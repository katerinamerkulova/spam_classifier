{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация текстов: спам-фильтр для SMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В настоящем проекте мы взяли открытый датасет с SMS-сообщениями, размеченными на спам (\"spam\") и не спам (\"ham\"), построили на нем классификатор текстов на эти два класса, оценили его качество посредством F1-меры, и посмотрели, что будет происходить с качеством, если менять параметры модели.\n",
    "\n",
    "1. Загружаем датасет. Описание датасета можно посмотреть здесь: https://www.kaggle.com/uciml/sms-spam-collection-dataset.\n",
    "\n",
    "2. Считываем датасет в Python, выясняя, что используется в качестве разделителей и как проставляются метки классов.\n",
    "\n",
    "3. Делим сообщения на 2 класса \"спам\" и \"не спам\", используя регулярные выражения. Создаем для дальнейшей работы список соответствующих меток классов. В качестве метки класса использовали 1 для спама и 0 для \"не спама\". Смотрим на (не)сбалансированность классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер выборки: 5573\n",
      "Количество не спама: 4826\n",
      "Количество спама: 747\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "file = open('SMSSpamCollection.txt', \"r\", encoding=\"utf-8\").read()\n",
    "\n",
    "ham = re.findall('ham\\t(.+)\\n', file)\n",
    "spam = re.findall('spam\\t(.+)\\n', file)\n",
    "\n",
    "print('Размер выборки:', len(ham+spam))\n",
    "print('Количество не спама:', len(ham))\n",
    "print('Количество спама:', len(spam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Делаем классы сбалансированными с помощью бутстрапа.\n",
    "5. Разделяем датасет на обучающую и тестовую выборки в пропорции 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "bs = resample(spam, n_samples=4826)\n",
    "data = ham + bs\n",
    "labels = [0]*len(ham) + [1]*len(bs)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Используя CountVectorizer со стандартными настройками, получаем из текстов матрицу признаков X.\n",
    "5. Классифицируем тексты с помощью LogisticRegression с параметрами по умолчанию, делаем предсказания и оцениваем их качество с помощью F1-меры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9945517582961863\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "clf_pipeline = Pipeline(\n",
    "            [(\"vectorizer\", vectorizer), (\"classifier\", logreg)])\n",
    "clf_pipeline.fit(X_train, y_train)\n",
    "y_pred = clf_pipeline.predict(X_test)\n",
    "print(f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Повторяем шаги из п.4 и п.5 для TfidfVectorizer. И замечаем, что качество ухудшается. Учитывая, что тексты не были предобаботаны (удаление знаков препинания и лемматизация слов), могла получиться довольна разреженная матрица признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.986679822397632\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "clf_pipeline = Pipeline(\n",
    "            [(\"vectorizer\", vectorizer), (\"classifier\", logreg)])\n",
    "clf_pipeline.fit(X_train, y_train)\n",
    "y_pred = clf_pipeline.predict(X_test)\n",
    "print(f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Задаем в CountVectorizer параметр ngram_range=(2,2), затем ngram_range=(3,3), затем ngram_range=(1,3). Во всех трех случаях измеряем получившееся значение F1-меры. В данном эксперименте мы пробуем добавлять в признаки n-граммы для разных диапазонов n - только биграммы, только триграммы, и, наконец, все вместе - униграммы, биграммы и триграммы. Обратим внимание, что статистики по биграммам и триграммам намного меньше, поэтому классификатор только на них работает хуже. В то же время это не ухудшает результат сколько-нибудь существенно, если добавлять их вместе с униграммами, т.к. за счет регуляризации линейный классификатор не склонен сильно переобучаться на этих признаках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2) 0.9975186104218362\n",
      "(3, 3) 0.9945246391239423\n",
      "(1, 3) 0.998019801980198\n"
     ]
    }
   ],
   "source": [
    "for var in [(2, 2), (3, 3), (1, 3)]:\n",
    "    vectorizer = CountVectorizer(ngram_range=var)\n",
    "    clf_pipeline = Pipeline(\n",
    "            [(\"vectorizer\", vectorizer), (\"classifier\", logreg)])\n",
    "    clf_pipeline.fit(X_train, y_train)\n",
    "    y_pred = clf_pipeline.predict(X_test)\n",
    "    print(var, f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод. Наибольшее качество (99,8%) в нашей задачи классификация показывает CountVectorizer с использованием сочетания униграмм, биграмм и триграмм слов в качестве признаков. Но для экономии ресурсов (времени и вычислительных мощностей) целесообразнее использовать только униграммы слов без значительной потери качества (99,45%) менее, чем на 1%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Бонус: примеры сообщений для теста:\n",
    "\n",
    "\"FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! Subscribe6GB\"\n",
    "\n",
    "\"FreeMsg: Txt: claim your reward of 3 hours talk time\"\n",
    "\n",
    "\"Have you visited the last lecture on physics?\"\n",
    "\n",
    "\"Have you visited the last lecture on physics? Just buy this book and you will have all materials! Only 99$\"\n",
    "\n",
    "\"Only 99$\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
